{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-Resolution GAN (SRGAN) for Astronomical Images\n",
    "\n",
    "This notebook provides a corrected and enhanced implementation of a Super-Resolution Generative Adversarial Network (SRGAN) tailored for astronomical data. It includes bug fixes, a proper GAN training loop, and metrics for evaluating image quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    "First, we'll install necessary libraries and import them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe7983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577eb556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, applications\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a12fde",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "Here we'll define functions to load and preprocess our data. For astronomical data, it's often grayscale, so we'll handle that. We'll also create low-resolution versions of the high-resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(data_path, hr_size, lr_size, num_images=100):\n",
    "    high_res_images = []\n",
    "    low_res_images = []\n",
    "    \n",
    "    image_files = [os.path.join(data_path, f) for f in os.listdir(data_path) if f.endswith(('jpg', 'png', 'jpeg'))]\n",
    "    image_files = image_files[:num_images]\n",
    "    \n",
    "    for image_path in tqdm(image_files, desc='Loading Images'):\n",
    "        hr_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if hr_img is None:\n",
    "            continue\n",
    "            \n",
    "        hr_img = cv2.resize(hr_img, (hr_size, hr_size))\n",
    "        lr_img = cv2.resize(hr_img, (lr_size, lr_size), interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        high_res_images.append(hr_img)\n",
    "        low_res_images.append(lr_img)\n",
    "        \n",
    "    high_res_images = np.array(high_res_images).astype(np.float32) / 255.0\n",
    "    low_res_images = np.array(low_res_images).astype(np.float32) / 255.0\n",
    "    \n",
    "    return low_res_images, high_res_images\n",
    "\n",
    "# Example usage:\n",
    "DATA_PATH = './astronomical_data/'\n",
    "HR_SIZE = 128\n",
    "LR_SIZE = 32\n",
    "lr_train, hr_train = load_images(DATA_PATH, HR_SIZE, LR_SIZE, num_images=200)\n",
    "\n",
    "# Placeholder data for demonstration without an actual dataset\n",
    "def generate_dummy_data(hr_size, lr_size, num_images=50):\n",
    "    hr_train = np.random.rand(num_images, hr_size, hr_size)\n",
    "    lr_train = np.array([cv2.resize(img, (lr_size, lr_size), interpolation=cv2.INTER_AREA) for img in hr_train])\n",
    "    return lr_train, hr_train\n",
    "\n",
    "HR_SIZE = 128\n",
    "LR_SIZE = 32\n",
    "lr_train, hr_train = generate_dummy_data(HR_SIZE, LR_SIZE)\n",
    "hr_train = np.expand_dims(hr_train, axis=-1)\n",
    "lr_train = np.expand_dims(lr_train, axis=-1)\n",
    "\n",
    "print(f\"Low-resolution training data shape: {lr_train.shape}\")\n",
    "print(f\"High-resolution training data shape: {hr_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3b7aba",
   "metadata": {},
   "source": [
    "## 3. Model Architecture\n",
    "### Generator\n",
    "The generator uses residual blocks to learn the mapping from low to high resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af255840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters):\n",
    "    res = x\n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.PReLU()(x)\n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.add([res, x])\n",
    "    return x\n",
    "\n",
    "def generator_model(lr_shape=(32, 32, 1), num_residual_blocks=16, upscale_factor=4):\n",
    "    filters = 64\n",
    "    \n",
    "    lr_input = layers.Input(shape=lr_shape)\n",
    "    \n",
    "    x = layers.Conv2D(filters, (9, 9), padding='same')(lr_input)\n",
    "    x = layers.PReLU()(x)\n",
    "    \n",
    "    res_input = x\n",
    "    \n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(x, filters)\n",
    "        \n",
    "    x = layers.Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.add([res_input, x])\n",
    "    \n",
    "    # Upsampling blocks\n",
    "    for _ in range(int(np.log2(upscale_factor))):\n",
    "        x = layers.Conv2D(filters * 4, (3, 3), padding='same')(x)\n",
    "        x = layers.Lambda(lambda z: tf.nn.depth_to_space(z, 2))(x)\n",
    "        x = layers.PReLU()(x)\n",
    "        \n",
    "    hr_output = layers.Conv2D(1, (9, 9), padding='same', activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs=lr_input, outputs=hr_output, name='Generator')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1531714e",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "The discriminator is a standard classifier that outputs a probability for whether an image is real or fake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6958a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model(hr_shape=(128, 128, 1)):\n",
    "    def d_block(x, filters, strides):\n",
    "        x = layers.Conv2D(filters, (3, 3), strides=strides, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "        return x\n",
    "    \n",
    "    hr_input = layers.Input(shape=hr_shape)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same')(hr_input)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    x = d_block(x, 64, 2)\n",
    "    x = d_block(x, 128, 1)\n",
    "    x = d_block(x, 128, 2)\n",
    "    x = d_block(x, 256, 1)\n",
    "    x = d_block(x, 256, 2)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
    "    \n",
    "    d_output = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    return Model(inputs=hr_input, outputs=d_output, name='Discriminator')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e123f8a8",
   "metadata": {},
   "source": [
    "## 4. Loss Functions\n",
    "We use a combination of content loss (MSE) and adversarial loss for the generator. Binary Cross-Entropy is used for the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6e6472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_losses():\n",
    "    mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    return mse_loss, bce_loss\n",
    "\n",
    "mse_loss, bce_loss = get_losses()\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = bce_loss(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = bce_loss(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output, generated_images, hr_images, content_loss_weight=1e-3):\n",
    "    adversarial_loss = bce_loss(tf.ones_like(fake_output), fake_output)\n",
    "    content_loss = mse_loss(hr_images, generated_images)\n",
    "    total_loss = content_loss_weight * content_loss + adversarial_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fee4853",
   "metadata": {},
   "source": [
    "## 5. Optimizers and Training\n",
    "We'll use separate optimizers for the generator and discriminator and define a custom training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccdaa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = Adam(learning_rate=1e-4)\n",
    "discriminator_optimizer = Adam(learning_rate=1e-4)\n",
    "\n",
    "generator = generator_model(lr_shape=(LR_SIZE, LR_SIZE, 1), upscale_factor=4)\n",
    "discriminator = discriminator_model(hr_shape=(HR_SIZE, HR_SIZE, 1))\n",
    "\n",
    "@tf.function\n",
    "def train_step(lr_images, hr_images):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(lr_images, training=True)\n",
    "        \n",
    "        real_output = discriminator(hr_images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output, generated_images, hr_images)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "        \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    \n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    \n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "def train(dataset, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        gen_total_loss = 0.0\n",
    "        disc_total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for lr_batch, hr_batch in tqdm(dataset, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            g_loss, d_loss = train_step(lr_batch, hr_batch)\n",
    "            gen_total_loss += g_loss\n",
    "            disc_total_loss += d_loss\n",
    "            num_batches += 1\n",
    "            \n",
    "        avg_gen_loss = gen_total_loss / num_batches\n",
    "        avg_disc_loss = disc_total_loss / num_batches\n",
    "        print(f'Epoch {epoch+1} - Generator Loss: {avg_gen_loss:.4f}, Discriminator Loss: {avg_disc_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a198f002",
   "metadata": {},
   "source": [
    "## 6. Evaluation Metrics\n",
    "We'll add PSNR and SSIM to measure the quality of the super-resolved images. These are essential for quantitative analysis. We will also visualize the metrics using seaborn and a few image comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(generator, lr_images, hr_images):\n",
    "    generated_images = generator.predict(lr_images)\n",
    "    \n",
    "    psnr_scores = []\n",
    "    ssim_scores = []\n",
    "    \n",
    "    for i in range(lr_images.shape[0]):\n",
    "        p = psnr(hr_images[i], generated_images[i], data_range=1.0)\n",
    "        s = ssim(hr_images[i], generated_images[i], data_range=1.0, channel_axis=-1)\n",
    "        \n",
    "        psnr_scores.append(p)\n",
    "        ssim_scores.append(s)\n",
    "    \n",
    "    avg_psnr = np.mean(psnr_scores)\n",
    "    avg_ssim = np.mean(ssim_scores)\n",
    "    \n",
    "    print(f\"\\nAverage PSNR: {avg_psnr:.4f}\")\n",
    "    print(f\"Average SSIM: {avg_ssim:.4f}\")\n",
    "    \n",
    "    # Visualize metrics using seaborn\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    sns.histplot(psnr_scores, kde=True, ax=axes[0])\n",
    "    axes[0].set_title('PSNR Distribution')\n",
    "    axes[0].set_xlabel('PSNR Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    \n",
    "    sns.histplot(ssim_scores, kde=True, ax=axes[1])\n",
    "    axes[1].set_title('SSIM Distribution')\n",
    "    axes[1].set_xlabel('SSIM Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return psnr_scores, ssim_scores\n",
    "\n",
    "def plot_comparisons(generator, lr_images, hr_images, num_images=3):\n",
    "    generated_images = generator.predict(lr_images[:num_images])\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        lr_img = lr_images[i].squeeze()\n",
    "        hr_img = hr_images[i].squeeze()\n",
    "        gen_img = generated_images[i].squeeze()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(lr_img, cmap='gray')\n",
    "        axes[0].set_title('Low Resolution')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(gen_img, cmap='gray')\n",
    "        axes[1].set_title('Super-Resolved')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(hr_img, cmap='gray')\n",
    "        axes[2].set_title('High Resolution (Ground Truth)')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc82d202",
   "metadata": {},
   "source": [
    "## 7. Main Execution\n",
    "Here we'll tie everything together and run the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a89af01",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((lr_train, hr_train))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "train(dataset, EPOCHS)\n",
    "\n",
    "print(\"\\nStarting evaluation...\")\n",
    "lr_test, hr_test = generate_dummy_data(HR_SIZE, LR_SIZE, num_images=10)\n",
    "lr_test = np.expand_dims(lr_test, axis=-1)\n",
    "hr_test = np.expand_dims(hr_test, axis=-1)\n",
    "\n",
    "psnr_scores, ssim_scores = evaluate_model(generator, lr_test, hr_test)\n",
    "\n",
    "print(\"\\nPlotting image comparisons...\")\n",
    "plot_comparisons(generator, lr_test, hr_test, num_images=3)\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
